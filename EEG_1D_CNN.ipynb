{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Imports"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "from keras.layers import Conv1D, AveragePooling1D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks  import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dense, Flatten, Dropout\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.callbacks  import EarlyStopping\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "#os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n",
      "Features : (7388, 42)\n",
      "Labels : (7388, 2)\n",
      "Number of classes : 2\n",
      "Unique labels: [0 1]\n"
     ]
    }
   ],
   "source": [
    "#dataPath = '../data/iir_feat_tensor_full_with_full_labels.mat'\n",
    "dataPath = '../data/spectral_feat_tensor_full_with_full_labels.mat'\n",
    "savePath = '../results/1D_CNN_Results/Spectral/'\n",
    "\n",
    "experiment = 'HVLV_Spectral_PD'\n",
    "\n",
    "filename = savePath+'CNN_1D_results_'+experiment+'.mat'\n",
    "plot_title = 'HV vs LV Classification for Spectral Features (PD)'\n",
    "\n",
    "nb_filters = [16, 32, 32, 64, 128]\n",
    "kernel_size = 3\n",
    "pool_size = 2\n",
    "stride_size = 2\n",
    "padding = 'same'\n",
    "weight_decay = 0.000001\n",
    "dense_layer_neuron_num = 128\n",
    "epochs = 30\n",
    "momentum =0.8\n",
    "\n",
    "matContent = sio.loadmat(dataPath)\n",
    "features = matContent['pd_feat']\n",
    "labels = np.squeeze(matContent['pd_hvlv_labels'])\n",
    "#labels[labels < 0] = 0\n",
    "print(labels)\n",
    "#labels[labels == 6] = 0\n",
    "#labels = labels.astype(int)\n",
    "\n",
    "#df = pd.read_csv(dataPath, header = None)\n",
    "#features = df.iloc[1:,:-2].to_numpy()\n",
    "#labels = df.iloc[1:,-1].to_numpy() # last but one column for PD vs NC\n",
    "\n",
    "#dict_hvlv = {1:0, 2:1, 3:0, 4:0, 5:1, 6:0} #HVLV labels mapping dictionary\n",
    "#labels = labels.map(dict_hvlv).to_numpy()\n",
    "#labels[labels == 1] = 0\n",
    "#labels[labels == 2] = 1\n",
    "#labels[labels == 3] = 1\n",
    "#labels[labels == 4] = 1\n",
    "#labels[labels == 5] = 1\n",
    "#labels[labels == 6] = 1\n",
    "#labels[labels < 0]=0\n",
    "#labels[labels == 6]=0\n",
    "\n",
    "# randomise the sample sequence\n",
    "rand_order = np.arange(features.shape[0])\n",
    "np.random.shuffle(rand_order)\n",
    "features = features[rand_order,]\n",
    "labels = np.squeeze(labels[rand_order,])\n",
    "class_num = np.size(np.unique(labels))\n",
    "labels_categorical = np_utils.to_categorical(labels, class_num)\n",
    "del matContent\n",
    "\n",
    "print('Features :', features.shape)\n",
    "print('Labels :', labels_categorical.shape)\n",
    "print('Number of classes :', class_num)\n",
    "print('Unique labels:',np.unique(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_model(init_mode, activation, dropout_rate, optimizer, learn_rate):\n",
    "#def create_model(activation):\n",
    "  model = Sequential()\n",
    "  model.add(Conv1D(filters=nb_filters[0], kernel_size=kernel_size, padding=padding,\n",
    "                   activation=activation, input_shape=(X.shape[1],X.shape[2]), trainable=True))\n",
    "  model.add(AveragePooling1D(pool_size=pool_size, strides=stride_size, padding=padding))\n",
    "  model.add(Conv1D(filters=nb_filters[1], kernel_size=kernel_size, padding=padding,\n",
    "                   activation=activation, kernel_initializer=init_mode, trainable=True))\n",
    "  model.add(AveragePooling1D(pool_size=pool_size, strides=stride_size, padding=padding))\n",
    "  model.add(Conv1D(filters=nb_filters[2], kernel_size=kernel_size, padding=padding,\n",
    "                   activation=activation, kernel_initializer=init_mode, trainable=True))\n",
    "  model.add(AveragePooling1D(pool_size=pool_size, strides=stride_size, padding=padding))\n",
    "  # ####added by me#####\n",
    "  #model.add(Conv1D(filters=nb_filters[3], kernel_size=kernel_size, padding=padding, activation=activation,\n",
    "  #              kernel_initializer='he_normal'))\n",
    "  #model.add(AveragePooling1D(pool_size=pool_size, strides=stride_size, padding=padding))\n",
    "  #model.add(Conv1D(filters=nb_filters[4], kernel_size=kernel_size, padding=padding, activation=activation,\n",
    "  #              kernel_initializer='he_normal'))\n",
    "  #model.add(AveragePooling1D(pool_size=pool_size, strides=stride_size, padding=padding))\n",
    "  # ####added by me#####\n",
    "  model.add(Flatten())\n",
    "  model.add(BatchNormalization(epsilon=0.001))\n",
    "  model.add(Dense(dense_layer_neuron_num, kernel_initializer=init_mode, activation=activation))\n",
    "  model.add(Dropout(dropout_rate))\n",
    "  model.add(Dense(class_num))\n",
    "  model.add(Activation('softmax'))\n",
    "  model.summary()\n",
    "  #model.load_weights('Gender_notClean_HIweights.hdf5')\n",
    "  #earlyStopping = EarlyStopping(monitor='val_loss', patience=2, verbose=1, mode='auto')\n",
    "  if optimizer == 'SGD':\n",
    "    opt = SGD(learning_rate=learn_rate / 10 ** epochs, momentum = momentum, decay = weight_decay, nesterov = True)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "  elif optimizer == 'Adam':\n",
    "    opt = Adam(learning_rate=learn_rate)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "  elif optimizer == 'RMSprop':\n",
    "    opt = RMSprop(learning_rate=learn_rate, epsilon=1e-07)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "  return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train and Test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(array(['relu'], dtype='<U4'), array([[0.3]]), array(['he_normal'], dtype='<U9'), array([[0.001]]), array(['RMSprop'], dtype='<U7'))]]\n"
     ]
    }
   ],
   "source": [
    "mat_path = '../results/CNN_1D/CNN_1D_CSP/CNN_1D_results_HVLV_csp_PD.mat'\n",
    "params = sio.loadmat(mat_path)\n",
    "best_params = params['best_params']\n",
    "del mat_path\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 42, 16)            64        \n",
      "_________________________________________________________________\n",
      "average_pooling1d (AveragePo (None, 21, 16)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 21, 32)            1568      \n",
      "_________________________________________________________________\n",
      "average_pooling1d_1 (Average (None, 11, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 11, 32)            3104      \n",
      "_________________________________________________________________\n",
      "average_pooling1d_2 (Average (None, 6, 32)             0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 192)               768       \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               24704     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 30,466\n",
      "Trainable params: 30,082\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Standardize\n",
    "\n",
    "feat_shape = features.shape\n",
    "#features = np.reshape(features, (feat_shape[0], feat_shape[1]*feat_shape[2]))\n",
    "#scaler = StandardScaler()\n",
    "#scaler.fit(features)\n",
    "#scaleFeatures = scaler.transform(features)\n",
    "features = np.reshape(features, (features.shape[0], 42, -1))\n",
    "\n",
    "X = features\n",
    "Y = labels_categorical\n",
    "tf.keras.backend.clear_session()\n",
    "estimator = create_model(init_mode='he_normal', learn_rate=0.0001, optimizer='Adam', activation='sigmoid', \n",
    "                         dropout_rate=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cnn_model = KerasClassifier(build_fn=create_model, verbose=1)  # Call the classifier\n",
    "\n",
    "#batch_size = [16,32]\n",
    "#epochs = [5,10,15]\n",
    "#optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "#init_mode = ['uniform', 'lecun_uniform', 'normal', 'zero', 'he_normal', 'he_uniform']\n",
    "#activation = ['softmax', 'softsign', 'relu', 'tanh', 'sigmoid', 'linear']\n",
    "#dropout_rate = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "#learn_rate = [0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "#momentum = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]\n",
    "#neurons = [1, 5, 10, 15, 20, 25, 30]\n",
    "\n",
    "learn_rate = [0.0001, 0.001, 0.01]\n",
    "optimizer = ['SGD', 'Adam','RMSprop']\n",
    "#momentum = [0.8,0.9]\n",
    "init_mode = ['he_normal','he_uniform']\n",
    "activation = ['relu','tanh']\n",
    "dropout_rate = [0.3,0.4,0.5]\n",
    "foldNum = 10\n",
    "\n",
    "p_grid = dict(init_mode=init_mode, dropout_rate=dropout_rate, activation=activation,\n",
    "              optimizer=optimizer, learn_rate=learn_rate)\n",
    "              #, momentum=momentum)\n",
    "grid = GridSearchCV(estimator=cnn_model, param_grid=p_grid,\n",
    "                    cv=foldNum, verbose=0)\n",
    "# Standerdize\n",
    "feat_shape = features.shape\n",
    "#features = np.reshape(features, (feat_shape[0], feat_shape[1]*feat_shape[2]))\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(features)\n",
    "scaleFeatures = scaler.transform(features)\n",
    "scaleFeatures = np.reshape(scaleFeatures, (features.shape[0], 640, 14))\n",
    "\n",
    "# Perform Grid Search\n",
    "print('Performing Gridsearch')\n",
    "X = scaleFeatures\n",
    "Y = labels_categorical\n",
    "grid_result = grid.fit(X,Y)\n",
    "best_params = grid_result.best_params_\n",
    "print('Best parameters:', best_params)\n",
    "tf.keras.backend.clear_session()\n",
    "estimator = create_model(init_mode=best_params.get('init_mode'), \n",
    "                         learn_rate=best_params.get('learn_rate'), \n",
    "                         optimizer=best_params.get('optimizer'), \n",
    "                         #momentum=best_params.get('momentum'), \n",
    "                         activation=best_params.get('activation'), \n",
    "                         dropout_rate=best_params.get('dropout_rate'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "foldNum = 5\n",
    "conf_mat = np.zeros((2,2))\n",
    "#conf_mat = np.zeros((6,6))\n",
    "f = 0\n",
    "val_loss = []\n",
    "val_accuracy = []\n",
    "train_loss = []\n",
    "train_accuracy = []\n",
    "f1_MacroNet = np.zeros([foldNum,]) \n",
    "f1_weightedNet = np.zeros([foldNum,])\n",
    "precisionNet = np.zeros([foldNum,])\n",
    "recallNet = np.zeros([foldNum,])\n",
    "accNet = np.zeros([foldNum,])\n",
    "\n",
    "#channels = features.shape[2] # number of channels 14\n",
    "# Use Stratified K Fold to make sure that the train data and test data split distribution match.\n",
    "kfold = StratifiedKFold(n_splits=5, random_state=100, shuffle=True)\n",
    "\n",
    "#features = np.reshape(features, (features.shape[0],640,14)) #reshaped to (None, 3, 14) for spectral\n",
    "\n",
    "cm_list = []\n",
    "for train, test in kfold.split(features, labels):\n",
    "    trainingFeatures = features[train,:,:]\n",
    "    testFeatures = features[test,:,:]\n",
    "    train_shape = trainingFeatures.shape\n",
    "    test_shape = testFeatures.shape\n",
    "    \n",
    "    # Standerdize \n",
    "    #scaler = StandardScaler()\n",
    "    #trainingFeatures = np.reshape(trainingFeatures, [train_shape[0], train_shape[1]*train_shape[2]])\n",
    "    #testFeatures = np.reshape(testFeatures, [test_shape[0], test_shape[1]*test_shape[2]])\n",
    "    #scaler.fit(trainingFeatures)\n",
    "    #trainingFeatures = scaler.transform(trainingFeatures)\n",
    "    #trainingFeatures = np.reshape(trainingFeatures, [train_shape[0],train_shape[1],train_shape[2]])\n",
    "    #testFeatures = scaler.transform(testFeatures)\n",
    "    #testFeatures = np.reshape(testFeatures,[test_shape[0], test_shape[1], test_shape[2]])\n",
    "    tf.keras.backend.clear_session()\n",
    "    estimator.summary()\n",
    "    X_train, X_val, y_train, y_val = train_test_split(trainingFeatures, labels_categorical[train,:], test_size=0.1,\n",
    "                                                      random_state=100, stratify=labels_categorical[train,:])\n",
    "    #earlyStopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1, mode='min')\n",
    "    dummy = estimator.fit(X_train, y_train, validation_data=(X_val, y_val), batch_size=32, epochs=epochs, \n",
    "                          verbose=2, validation_split=0.1)\n",
    "    \n",
    "    predicted_labelsNet = estimator.predict_classes(testFeatures, verbose=0)\n",
    "    cm = confusion_matrix(labels[test,], predicted_labelsNet, labels=[0,1])\n",
    "    #cm = confusion_matrix(labels[test,], predicted_labelsNet, labels=[1,2,3,4,5,0])\n",
    "    cm = cm/cm.sum(axis=1, keepdims=True)\n",
    "    #conf_mat = conf_mat+cm\n",
    "    cm_list.append(cm)\n",
    "\n",
    "    precisionNet[f] = precision_score(labels[test,], predicted_labelsNet, average='macro')\n",
    "    recallNet[f] = recall_score(labels[test,], predicted_labelsNet, average='macro')\n",
    "    f1_MacroNet[f] = f1_score(labels[test,], predicted_labelsNet, average='macro')\n",
    "    f1_weightedNet[f] = f1_score(labels[test,], predicted_labelsNet, average='weighted')\n",
    "    accNet[f] = accuracy_score(labels[test,], predicted_labelsNet)\n",
    "    print(experiment + '_CNN: Fold %d : f1_macroscore: %.4f' % (f + 1, f1_MacroNet[f]))\n",
    "    print(experiment + '_CNN: Fold %d : f1_weightedscore: %.4f' % (f + 1, f1_weightedNet[f]))\n",
    "    print(experiment + '_CNN: Fold %d : acc: %.4f' % (f + 1, accNet[f]))\n",
    "    f += 1\n",
    "    train_loss.append(dummy.history['loss'])\n",
    "    val_loss.append(dummy.history['val_loss'])\n",
    "    train_accuracy.append(dummy.history['accuracy'])\n",
    "    val_accuracy.append(dummy.history['val_accuracy'])\n",
    "\n",
    "#xc = range(1,epochs+1)\n",
    "tr_loss_df = pd.DataFrame(train_loss)\n",
    "tr_acc_df = pd.DataFrame(train_accuracy)\n",
    "val_loss_df = pd.DataFrame(val_loss)\n",
    "val_acc_df = pd.DataFrame(val_accuracy)\n",
    "\n",
    "train_loss_mlist = tr_loss_df.mean(axis=0).to_numpy()\n",
    "train_loss_slist = tr_loss_df.std(axis=0).to_numpy()\n",
    "train_acc_mlist = tr_acc_df.mean(axis=0).to_numpy() \n",
    "train_acc_slist = tr_acc_df.std(axis=0).to_numpy() \n",
    "val_loss_mlist = val_loss_df.mean(axis=0).to_numpy() \n",
    "val_loss_slist = val_loss_df.std(axis=0).to_numpy() \n",
    "val_acc_mlist = val_acc_df.mean(axis=0).to_numpy()\n",
    "val_acc_slist = val_acc_df.std(axis=0).to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Visualise results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.subplot(1, 2, 1)\n",
    "\n",
    "y1 = train_loss_mlist\n",
    "e1 = train_loss_slist\n",
    "y2 = val_loss_mlist\n",
    "e2 = val_loss_slist\n",
    "x1 = range(1, len(train_loss_mlist) + 1)\n",
    "a1 = plt.plot(x1, y1, color='#089FFF', label='Training Loss')\n",
    "plt.fill_between(x1, y1 - e1, y1 + e1, alpha=0.3, facecolor='#089FFF', linewidth=4)\n",
    "a2 = plt.plot(x1, y2, color='#cc9106', label='Validation Loss')\n",
    "plt.fill_between(x1, y2 - e2, y2 + e2, alpha=0.3, facecolor='#cc9106', linewidth=4)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "y3 = train_acc_mlist\n",
    "e3 = train_acc_slist\n",
    "y4 = val_acc_mlist\n",
    "e4 = val_acc_slist\n",
    "x2 = range(1, len(train_acc_mlist) + 1)\n",
    "a1 = plt.plot(x2, y3, color='#089FFF', label='Training Accuracy')\n",
    "plt.fill_between(x2, y3 - e3, y3 + e3, alpha=0.3, facecolor='#089FFF', linewidth=4)\n",
    "a2 = plt.plot(x2, y4, color='#cc9106', label='Validation Accuracy')\n",
    "plt.fill_between(x2, y4 - e4, y4 + e4, alpha=0.3, facecolor='#cc9106', linewidth=4)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "#plt.savefig(filename[:-4]+'_loss_acc_curve'+'.png')\n",
    "plt.show()\n",
    "\n",
    "#conf_mat /= conf_mat.sum(axis=1, keepdims=True) #Normalised values in the CM\n",
    "conf_mat = np.mean(cm_list, axis=0)\n",
    "#fig, ax = plt.subplots(figsize=(10,8))\n",
    "ax = sns.heatmap(conf_mat, cmap='YlGnBu', annot=True, fmt='.4f', vmin=0, vmax=1, annot_kws={'fontsize': 12})\n",
    "ax.set_yticklabels(['NC', 'PD'], rotation=0)\n",
    "ax.set_xticklabels(['NC', 'PD'], rotation=0)\n",
    "#ax.set_yticklabels(['Sad', 'Happy', 'Fear', 'Disgust', 'Surprise', 'Anger'], rotation = 0)\n",
    "#ax.set_xticklabels(['Sad', 'Happy', 'Fear', 'Disgust', 'Surprise', 'Anger'], rotation = 0)\n",
    "\n",
    "ax.set_title(plot_title)\n",
    "ax.get_figure().savefig(filename[:-4] + '_conf_mat' + '.png')\n",
    "plt.show()\n",
    "\n",
    "print('Mean and std of F1 MACRO is %.4f +- %.4f' % (np.mean(f1_MacroNet), np.std(f1_MacroNet)))\n",
    "print('Mean and std of F1 WEIGHTED is %.4f +- %.4f' % (np.mean(f1_weightedNet), np.std(f1_weightedNet)))\n",
    "print('Mean and std of accuracy is %.4f +- %.4f' % (np.mean(accNet), np.std(accNet)))\n",
    "\n",
    "# Save results\n",
    "sio.savemat(filename, {'precisionNet': precisionNet, 'recallNet': recallNet, 'f1_MacroNet': f1_MacroNet,\n",
    "                       'f1_weightedNet': f1_weightedNet, 'accNet': accNet, 'conf_mat': conf_mat,\n",
    "                       'conf_mat_list': cm_list,\n",
    "                       'best_params': best_params, 'experiment': experiment, 'nb_filters': nb_filters,\n",
    "                       'kernel_size': kernel_size, 'pool_size': pool_size, 'stride_size': stride_size,\n",
    "                       'padding': padding,\n",
    "                       'weight_decay': weight_decay, 'dense_layer_neuron_num': dense_layer_neuron_num, 'epochs': epochs,\n",
    "                       'train_loss': train_loss, 'train_accuracy': train_accuracy, 'val_loss': val_loss,\n",
    "                       'val_accuracy': val_accuracy})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}